# Relatório de Estudos

**Nome do Estagiário:** Carlos Eduardo Pereira de Oliveira 
**Data:** 08/08/2024

**Módulos/Etapas Feitas:**  
1. **Mensageria**
2. **Virtualização**
3. **Computação em Nuvem**


## Resumo dos módulos 

### **Mensageria**

Mensageria é  um conceito que define que sistemas distribuidos possam se comunicar por meio de troca de mensagens (evento), sendo estas mensagens gerenciadas por um Message Broker (servidor modulo de mensagens).

O conceito de mensageria que faz o uso de algum software, de algum sistema, é que dois sistemas diferentes vão se comunicar de modo atraves de trocas de mensagem. Uma requisição htpp é uma troca de mensagem, esse tipo de troca de mensagem funciona de forma assíncrona, por exemplo, se tiver um sistema A e sistema B, e eles precisam se comunicar, só que o sistema A não precisa esperar a resposta do B para continuar funcionando, se eu quiser enviar um email, é so falar para o sistema enviar este email, não precisa ficar esperando que envie o email, ou se chegou na caixa de entrada, caso de um erro no envio, ele avisa depois.

Pub/Sub é o conceito onde temos dois papeis, um de publiser que publica o evento ou uma mensagem e subscriber quem vai consumir e ler essa mensagem.
Esse serviço de mensagens é a escolha padrão para a maioria dos usuários e aplicativos. Ele oferece a maior confiabilidade e o maior conjunto de integrações, além do gerenciamento automático de capacidade. É compatível com a replicação síncrona de todos os dados para pelo menos duas zonas e a replicação de melhor esforço para uma terceira zona adicional. Combina a escalonabilidade horizontal do Apache Kafka e do Pulsar com recursos encontrados no middleware de mensagens, como Apache ActiveMQ e RabbitMQ. Exemplos desses recursos são filas de mensagens inativas e filtragem.

### **Virtualização**

Quando falamos sobre virtualização, estamos falando de máquina virtual que não é diferente de um computador físico, como um laptop, um smartphone ou um servidor. Tem tudo que uma máquina física tem, tem memória, conecta-se a internet. O hypervisor é uma camada de virtualização do tipo 1 chamado de bare metal é o que está colado no hardware, é o mais utilizado em aplicação de servidores pela capaciade ser maior. Tem também o tipo 2 é um sistema operacional separando o hypervisor do hardware

Os beneficios são o custos operacionais e de capital reduzido, agilidade mais velocidadae, redução de downtime - migração Live VM, mover um sistema operacional e aplicativos em execução do hardware antido para o novo, consolidação VM - vários aplicativos OS mais em uma única máquina física.

A máquina virtual é uma tecnologia é essencial para a area de TI, oferecendo escabilidade, flexibilidade e para muito aplicações de serviços.

Docker é um software que oferece ambiente virtualizados que impacotam todo um sistema operacional em sua aplicação para dentr de um conteiner, os conteiners são isolados um dos outros e agrupam seus proprios softwares e bibliotecas e arquivos de configuração, tornando esse conteiner portavel para ser utilizado em qualquer outro computador ou outro servidor. Com o docker consegue definir qual sistemas opercaional a versao e com dependencias exatas, escolhendo, o sistema sera capaz de rodar em qualquer lugar em um contaeiner docker, o docker virtualiza a aplicação. Da para se criar as imagens com as configurações e compartilhar essa imagem com as pessoas que queiram usar. Da tambem para compartilhar no dockerhub, existem bases de imagens, oficiais e disponibilizado pela comunidade, imagens de pyhton, conteiner, nodejs.

Kubernetes é um sistema de opersource para orquestraçao de containers para automatização de deploys e escalonamento e gerenciamento de aplicações.

O kubernetes foi pensando na melhor performance, na escabilidade e também na otimização dos recursos, utilizados para rodar as aplicações. Ele trabalha com muitas atualizações de uma aplicação, é possivel ir alternando um novo release de uma aplicação ao poucos com muito mais frequência e isso se encaixe perfeitamente com as metódologias ágeis atuais, onde a entrega das novas versões acontece a todos instante, em muitos casos, em diversas vezes ao dia, portanto, o orquestrador consegue viabilizar isso de forma automatizada. Com kubernetes da para agendar a atualização e se der errado ele consegue voltar o que era automaticamente.

O kubernetes é feito para trabalhar em um conjuto de containers, com 2 ou mais containers, isso representa, a criação de clusters, isso é, que pode ter vários containers rodando em uma só aplicação, em uma rede propria balaceando as requisições, fazendo com que uma apicação aumente rapidamente e exponencialmente a sua capaciade de atender suas requisições. Essa criação e remoção de cluster em um container acontece se modo programado, isso quer dizer que quando requer mais recurso, o orquestrador pode criar mais containers.

Permitem que os serviços descubram e consumam outros serviços sem fazer uso dos agentes de aplicações, tem abstração que encapsula o processo de upgrade de reversão de grupos de containers e tornan-se deploy automatizado e repetível.

### **Computação em Nuvem**

O Google Dataflow é um seviço de processamento de dado sem stream ou batch, o provisionamento, gerenciamento e escalonamento desse recurso é automático, e o desenvolvimento dessas papelaines é feito utilizando o apache beam SDK, que esta disponivel nas linguagens python, java, go, e outras.

Uma pipeline em batch temos o, data source gereando dados e disponibilizando eles em um bucket no google cloud storage, entao o dataflow consome os dados deste buckete faz o tatamento e processamento que foram definidos na pipeline e disponibiliza esses dados de forma estruturadas no google big query.

Uma pipeline streaming onde o apache kafka esta disponibilizando os dados em um tópico no google Pub/Sub, então o dataflow esta consumindo dados de um subscription atrelada ao tópico pub/sub, o google dataflow estara fazendo os tratamentos e as regras de transformações necessarias e também disponibilizados os dados para o bigquery.

vantagens em utilizar o google dataflow é a configiuração rápida e simplificada de pipelaines, oferece uma redução de custo com o emprego otimizado de workers, e também um recurso que faz processamento sem servidor, basicamente um processamento sem sevidor, é que os kluster são criados no momento de execução e após a execução eles são destruídos, ou seja, possibilitando assim, uma redução de custo.

DataProc é um serviço de cluster gerenciado dentro do gcp que é usado para executar trabalhos spark e hadoop. Ele elimina os principais elementos de gerenciamento de manipulação de clusters vms e armazenamento por conta propria e assume essas tarefas para que a pesoa possa se preocupar so com o seu trabalho, dataproc confugurará o serviço gerenciado  e tudo o que você terá que fazer, além desse ponto é enviar trabalho para ele, mas confirar clusters hadoop e spark é muito complicado. primeiro deve configurar a VM e depois alocar o armazenamento, o processamento e a memória, então terá que instala o software na vm e entao terá que repetir esse process algumas vezes para fazer o cluster. Assim deve repetir esse ciclo para aumentar o cluster.

Os serviços gerenciados na nuvem é a melhor escolha. Tem uma integração muito boa com o BigQuery, Cloud Storage, e Cloud Pub/Sub, facilitando a ingestão, o processamento e a análise de dados.

Google Cloud Composer é um serviço gerenciado de orquestração de workflows baseado em Apache Airflow, que ajuda você cronograma, monitorar e gereniar fluxos de trabalho. A automatização do compositor de nuvem ajuda a criar ambientes de fluxo e rapidamente e usar ferramentas nativas de fluxos como a interface web, ferramentas de linha de comando para que possa encontrar no fluxo de seu trabalhoe não sua infraestrutura. O apache airflow é uma plataforma de código aberto para orquestrar pipelines de dados programaticamente, principalmente com python, pode crias os fluxos de trabalho no apache airflow usando gráficos acíclicos direcionados ou dags. Um dos exemplos de fluxo de trabalho de análise de dados inclui a ingestão dados aplicando transformações e depois analisando os, um gráfico acíclico direcionado ou dag, definem as tarefas e dependencias que fazem parte de seu fluxo de trabalho e é escrito usando python. O objetivo principal de uma dag é apenas exucutar e no momento certo, na ordem certa e com tratamento correto dos problemas.

Quando cria um composer de trabalho em um ambiente no google cloud é criado automaticamente, a plataforma de airflow é baseada em uma arquitetura de microserviços em execução no mecanismo google kubernetes. E executa cada tarefa usando um api do mecanismo de computação, o agendador do airflow garante  que os backups sejam executados na cadência da configuração e com a ordem de tarefa adequada, o redis é usado como um intermediario de mensagens ntre o airflow, o proxy cloud sql de confiança é usado para se comunicar com o repositorio de meta dados.

O Google Cloud Functions é a solução de computação sem servidor do Google para criar aplicativos baseados em eventos. É um produto conjunto entre as equipes do Google Cloud Platform e do Firebase. Para os desenvolvedores do Google Cloud Platform, o Cloud Functions funciona como uma camada de conexão. E permite que você acione seu código no Google Cloud, Firebase e Google Assistente, ou o chame diretamente de qualquer aplicativo da Web, de dispositivos móveis ou do back-end via HTTP. Aumenta os serviços em nuvem atuais e permite atender a um número cada vez maior de casos de uso com lógica de programação arbitrária.Oferece mais flexibilidade e confiabilidade, desempenho e eficiência melhorados e ajuda a reduzir os custos de TI.


## Links de Laboratórios (se houver)
- [Youtube - Mensageria](https://www.youtube.com/watch?v=U5h6B7eSiAE)
- [Google Cloud](https://cloud.google.com/pubsub/docs/overview?hl=pt-br)
- [Máquina Virtual](https://azure.microsoft.com/pt-br/resources/cloud-computing-dictionary/what-is-a-virtual-machine)
- [Virtualização](https://www.youtube.com/watch?v=WVBCliA0PCs)
- [Kubernetes](https://www.youtube.com/watch?v=mVL0nOM3AGo)
- [DataFlow](https://www.youtube.com/watch?v=b0v0XzHZrVU)
- [DataProc](https://cloud.google.com/dataproc-serverless/docs?hl=pt-br)
- [Composer](https://cloud.google.com/docs?hl=pt-br)
- [Functions](https://cloud.google.com/functions?hl=pt_br)

**Recursos Utilizados:**  
- Google Cloud
- Google DataFlow

**Principais comandos: (se aplicável)**  
- FROM
- ENV
- WORKDIR
- RUN
- COPY
- docker Build

**Desafios Encontrados:**  
Entender todos os assuntos.

**Feedback e Ajustes:**  
Após ver o video da trilha de conhecimento, pude entender esses conceitos.
Com o template ajudou bastante a organizar o relatório.

**Próximos Passos:**  
Seguir estudando e praticando para se desenvolver.
Continuar a trilha de estudo.